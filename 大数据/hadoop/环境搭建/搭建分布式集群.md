## 前置条件

- [CentOS Linux release 7.9.2009](https://mirrors.cloud.tencent.com/centos/7.9.2009/isos/x86_64/CentOS-7-x86_64-Minimal-2009.iso)
- [JDK 1.8.0_202](https://repo.huaweicloud.com/java/jdk/8u202-b08/jdk-8u202-linux-x64.tar.gz)
- [Hadoop 3.2.0](http://archive.apache.org/dist/hadoop/common/hadoop-3.2.0/hadoop-3.2.0.tar.gz)



## 机器规划

| 别名     | IP              | 角色   | 安装服务                                     |
| -------- | --------------- | ------ | -------------------------------------------- |
| hadoop01 | 192.168.216.149 | 主节点 | ResourceManager、SecondaryNameNode、NameNode |
| hadoop02 | 192.168.216.148 | 从节点 | DataNode 、NodeManager                       |
| hadoop03 | 192.168.216.150 | 从节点 | DataNode 、NodeManager                       |



## 所有机器初始化

```bash
# 安装软件
yum install vim ntp -y 

# 禁用防火墙
systemctl disable firewalld --now 

# 设置Hosts信息
echo '192.168.216.149 hadoop01' >> /etc/hosts 
echo '192.168.216.148 hadoop02' >> /etc/hosts 
echo '192.168.216.150 hadoop03' >> /etc/hosts 

# 测试联通性
ping hadoop01
ping hadoop02
ping hadoop03

# 生成私钥
ssh-keygen -t rsa 

# 免密登录
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys 

# 时间同步
ntpdate ntp.aliyun.com 

# 设置定时进行时间同步
echo '* * * * * root /usr/sbin/ntpdate -u ntp.aliyun.com' >> /etc/crontab
```



## 所有机器安装JDK

```bash
# 创建目录
mkdir -p /opt/soft 

# 解压JDK 
tar -zxvf jdk-8u202-linux-x64.tar.gz -C /opt/soft 

# 设置环境变量
echo '# Java Env' >> /etc/profile 
echo "export JAVA_HOME=/opt/soft/jdk1.8.0_202" >> /etc/profile 
echo 'export JRE_HOME=$JAVA_HOME/jre' >> /etc/profile 
echo 'export CLASSPATH=.:$JAVA_HOME/lib' >> /etc/profile 
echo 'export PATH=$PATH:$JAVA_HOME/bin' >> /etc/profile 

# 激活
source /etc/profile 

# 测试
java -version
javac -version
```



## 主机之间免密登录

```bash
ssh-copy-id ~/.ssh/id_rsa.pub root@hadoop01 
ssh-copy-id ~/.ssh/id_rsa.pub root@hadoop02 
ssh-copy-id ~/.ssh/id_rsa.pub root@hadoop03
```



## 所有机器安装hadoop

```bash
# 解压
tar -zxvf hadoop-3.2.0.tar.gz -C /opt/soft 

# 设置环境变量
echo 'export HADOOP_HOME=/opt/soft/hadoop-3.2.0' >> /etc/profile 
echo 'export PATH=$PATH:$HADOOP_HOME/bin' >> /etc/profile 
echo 'export PATH=$PATH:$HADOOP_HOME/sbin' >> /etc/profile 
source /etc/profile 

# 配置
echo 'export JAVA_HOME=/opt/soft/jdk1.8.0_202' >> /opt/soft/hadoop-3.2.0/etc/hadoop/hadoop-env.sh 
echo 'export HADOOP_LOG_DIR=/data/hadoop_repo/logs/hadoop' >> /opt/soft/hadoop-3.2.0/etc/hadoop/hadoop-env.sh

# 同步配置到其他节点
scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers hadoop02:/opt/soft/hadoop-3.2.0/etc/hadoop 

scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers hadoop03:/opt/soft/hadoop-3.2.0/etc/hadoop

scp start-dfs.sh stop-dfs.sh start-yarn.sh stop-yarn.sh hadoop02:/opt/soft/hadoop-3.2.0/sbin
scp start-dfs.sh stop-dfs.sh start-yarn.sh stop-yarn.sh hadoop03:/opt/soft/hadoop-3.2.0/sbin
```



## 配置

```
core-site.xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop01:9000</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/data/hadoop_repo</value>
   </property>
</configuration>


hdfs-site.xml 
<configuration>
    <property>
        <name>dfs.replication</name>
        <value>2</value>
    </property>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop01:50090</value>
    </property>
</configuration>



mapred-site.xml
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>




yarn-site.xml
<configuration>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
	<property>
		<name>yarn.resourcemanager.hostname</name>
		<value>hadoop01</value>
	</property>
</configuration>



workers
hadoop02
hadoop03






start-dfs.sh
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root


stop-dfs.sh
HDFS_DATANODE_USER=root
HDFS_DATANODE_SECURE_USER=hdfs
HDFS_NAMENODE_USER=root
HDFS_SECONDARYNAMENODE_USER=root


start-yarn.sh
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root


stop-yarn.sh
YARN_RESOURCEMANAGER_USER=root
HADOOP_SECURE_DN_USER=yarn
YARN_NODEMANAGER_USER=root
```



## 同步配置到其他节点

```bash
scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers hadoop02:/opt/soft/hadoop-3.2.0/etc/hadoop 

scp hadoop-env.sh core-site.xml hdfs-site.xml mapred-site.xml yarn-site.xml workers hadoop03:/opt/soft/hadoop-3.2.0/etc/hadoop

scp start-dfs.sh stop-dfs.sh start-yarn.sh stop-yarn.sh hadoop02:/opt/soft/hadoop-3.2.0/sbin
scp start-dfs.sh stop-dfs.sh start-yarn.sh stop-yarn.sh hadoop03:/opt/soft/hadoop-3.2.0/sbin
```



## 启动服务

```bash
# 初始化
./bin/hdfs namenode -format

# 启动集群
./sbin/start-all.sh

# 关闭集群
./sbin/stop-all.sh
```



## 检查

```bash
三台机器都执行jps

一个主节点 
ResourceManager 
SecondaryNameNode 
NameNode 

两个从节点 
DataNode 
NodeManager

# 访问
http://[主节点IP]:9870/dfshealth.html#tab-overview 
http://[主节点IP]:8088/cluster 
```



​     